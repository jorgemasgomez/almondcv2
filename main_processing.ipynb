{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch images processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from calibrations import  build_calibration, calibrate_color_and_distortion, calibrate_color\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from aux_functions import slicing, obtain_pixel_metric, divide_in_sets, ungroup_pic\n",
    "from model_class import model_segmentation\n",
    "from pictures_class import pictures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs\n",
    "working_directory=\"C:/Users/Pheno/Documents/database_almondcv2/\"\n",
    "raw_folder=\"C:/Users/Pheno/Documents/database_almondcv2/2023/all_sessions_2023/errores\"\n",
    "chessboards=\"C:/Users/Pheno/Documents/database_almondcv2/calibracion/chessboards\"\n",
    "mtx_input_path=os.path.join(chessboards,\"calibration_mtx.npz\") ## for distorsion\n",
    "standard_matrix_color=\"C:/Users/Pheno/Documents/database_almondcv2/2023/all_sessions_2023/RGB_SEED_0509_2024-09-05-12-29-37_2.jpg\" ### a picture to select if some pictures produces errors in color \n",
    "info_data_2022=\"C:/Users/Pheno/Documents/database_almondcv2/info_data.txt\"\n",
    "info_data_2023=\"C:/Users/Pheno/Documents/database_almondcv2/info_data_2023.txt\"\n",
    "\n",
    "zip_file_coin=os.path.join(working_directory,\"coin_640_slices_2023set.zip\")\n",
    "zip_file_group=os.path.join(working_directory,\"rectangles_set_2023_shell.zip\")\n",
    "zip_file_shell=os.path.join(working_directory,\"shell_2022_320.zip\")\n",
    "zip_file_seed=os.path.join(working_directory,\"seed_2022_21102024.zip\")\n",
    "\n",
    "\n",
    "models_directory=\"C:/Users/Pheno/Documents/database_almondcv2/models/\"\n",
    "pre_model=os.path.join(models_directory, \"yolo11s-seg.pt\")\n",
    "\n",
    "coin_model_path=os.path.join(models_directory,\"coin_2023_yolov11s_640.pt\")\n",
    "\n",
    "group_model_path_1=os.path.join(models_directory, \"rectangles_shell_2023_imgsize_1280.pt\")\n",
    "group_model_path_2=os.path.join(models_directory, \"rectangles_seed_2023_imgsz_1600.pt\")\n",
    "group_model_path_2022=\"\"\n",
    "\n",
    "shell_model_path=os.path.join(models_directory, \"shell_2023_yolov11s_320.pt\")\n",
    "seed_model_path=os.path.join(models_directory, \"seed_2023_yolov11s_320.pt\")\n",
    "#outputs\n",
    "output=\"C:/Users/Pheno/Documents/database_almondcv2\"\n",
    "output_calibrated=os.path.join(output,\"calibrated_pics_2022/\")\n",
    "os.makedirs(output_calibrated, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color and distorsion correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to create distortion model\n",
    "build_calibration(chessboardSize=(6, 8), frameSize=(5472,3648), dir_path=chessboards, \n",
    "                  image_format=\".jpg\", size_of_chessboard_squares_mm=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora mismo esta puesto en modo calibracion los errores utilizando una foto como referencia, para calibracion normal quitar la standard_matrix_color\n",
    "calibrate_color_and_distortion(raw_folder=raw_folder,mtx_input_path=mtx_input_path,output_calibrated=output_calibrated,\n",
    "                                radius_param=10, standard_matrix=standard_matrix_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Si solo quieres hacer color esta la función\n",
    "\n",
    "# calibrate_color(input_folder=raw_folder, output_path=output_calibrated,standard_matrix=standard_matrix_color)\n",
    "calibrate_color(input_folder=raw_folder, output_path=output_calibrated,standard_matrix=standard_matrix_color,\n",
    "                 force_standard_matrix=\"Yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHUNK para mover automaticamente lo que este en la lista errores a donde tu queiras\n",
    "#first we move \n",
    "\n",
    "carpeta_destino = os.path.join(raw_folder,\"errores\")    # Cambia esto por la ruta correcta\n",
    "\n",
    "# Crea la carpeta de destino si no existe\n",
    "os.makedirs(carpeta_destino, exist_ok=True)\n",
    "\n",
    "\n",
    "# Lee el archivo de texto con los nombres de las imágenes\n",
    "with open(os.path.join(output_calibrated,\"errors_in_calibrations.txt\"), \"r\") as archivo:\n",
    "    for nombre_imagen in archivo:\n",
    "        nombre_imagen = nombre_imagen.strip()  # Elimina espacios en blanco y saltos de línea\n",
    "        ruta_imagen_origen = os.path.join(raw_folder, nombre_imagen)\n",
    "        \n",
    "        # Verifica si el archivo existe antes de moverlo\n",
    "        if os.path.exists(ruta_imagen_origen):\n",
    "            # Mueve la imagen a la carpeta de destino\n",
    "            shutil.copy(ruta_imagen_origen, carpeta_destino)\n",
    "            print(f\"Moviendo: {nombre_imagen} a {carpeta_destino}\")\n",
    "        else:\n",
    "            print(f\"No se encontró la imagen: {nombre_imagen}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load info data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar info_data, se puede cargar ya una columna pixelmetric con la informacion pixel-mm o obtener la metrica pixel desde las imagenes.\n",
    "info_data_df=pd.read_csv(info_data,sep=\"\\t\")\n",
    "# Si las hemos calibrado añadimos CL delante\n",
    "info_data_df['Name_picture'] = info_data_df['Name_picture'].apply(lambda x: 'CL_' + x)\n",
    "info_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain pixel_metric from the pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si no tienes un modelo para reconocer tu objeto de referencia debes crearlo\n",
    "#Se comienza haciendo particiones de la imagen con la función slicing para introducirlas en CVAT\n",
    "\n",
    "slicing(input_folder=output_calibrated,output_directory=working_directory,\n",
    "        name_slicing=\"gsgsg\", number_pictures=60,\n",
    "          crop=\"left\", slice_height=640, slice_width=640, overlap_height_ratio=0.2, overlap_width_ratio=0.2, crop_level=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posteriormente se tiene que segmentar en CVAT, y ese output se leera para entrenar el modelo y ver si funciona bien, para ello utilizamos la función\n",
    "# train model, pon el archivo.zip en el working directory.\n",
    "\n",
    "coin_model=model_segmentation(working_directory=working_directory)\n",
    "coin_model.train_segmentation_model(input_zip=zip_file_coin, epochs=80,imgsz=640,\n",
    "                                     name_segmentation=\"coin_640px_slices640_26102024_set2023_retina\", pre_model=pre_model\n",
    "                                     , batch=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si tenemos el modelo ya simplemente podemos cargarlo y aplicarlo sobre la carpeta que queramos\n",
    "\n",
    "coin_model_saved=model_segmentation(working_directory=working_directory)\n",
    "contours_coin=coin_model_saved.slice_predict_reconstruct(input_folder=output_calibrated, imgsz=640,\n",
    "                                                         model_path=coin_model_path, slice_height=640, slice_width=640,\n",
    "                                                         overlap_height_ratio=0.2, overlap_width_ratio=0.2,\n",
    "                                                           retina_mask=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_data_completed=obtain_pixel_metric(info_data=info_data_df, contours=contours_coin,\n",
    "                                         output_directory=working_directory, reference=24.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_data_completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desagrupar imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una vez hemos completado ya el pixelmetric, si tenemos fotos agrupadas, debemos desagruparlas y conectarlas con su ID\n",
    "#Para posteriormente sacar las medidas que queramos. Esta vez no hacemos slicing, peusto que los grupos son mas grandes que las slcies.\n",
    "# divide_in_sets(input_folder=output_calibrated,output_directory=working_directory, division_name=\"rectangle_group_2023\", number_pictures=100)\n",
    "\n",
    "#Esta vez hacemos solo shell\n",
    "folder_shell=os.path.join(working_directory, \"Seed_2023_set\")\n",
    "divide_in_sets(input_folder=folder_shell,output_directory=working_directory, division_name=\"rectangle_group_2023_seed\", number_pictures=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora vamos a entrenar un modelo para que reconozca los grupos\n",
    "\n",
    "group_model=model_segmentation(working_directory=working_directory)\n",
    "group_model.train_segmentation_model(input_zip=zip_file_group, epochs=80,imgsz=1280,\n",
    "                                      name_segmentation=\"rectangle_group_2023set_shell\", pre_model=pre_model, batch=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESTO DESPUES PARA LOS CONTORNOS EN GROUPED PICTURES\n",
    "\n",
    "# Si tenemos el modelo ya simplemente podemos cargarlo y aplicarlo sobre la carpeta que queramos\n",
    "#Esta funcion se puede optimizar, cuando hay muchas fotos se llena la RAM y va mas lento a partir de las 300 fotos.Habría que hacer tandas, lo que lo peta es la retina mask. \n",
    "group_model_saved=model_segmentation(working_directory=working_directory)\n",
    "contours_groups=group_model_saved.predict_model(model_path=group_model_path_2,\n",
    "                               folder_input=os.path.join(working_directory, \"Seed_2023_set\"),\n",
    "                               imgsz=1600, check_result=False, max_det=2, retina_mask=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora que ya tenemos los contornos, podemos aplicarlo a nuestras imagenes, y tambien coordinaar el info_data_df\n",
    "info_data_completed_path=os.path.join(working_directory, \"info_data_completed_2023.txt\")\n",
    "info_data_completed=pd.read_csv(info_data_completed_path,sep=\"\\t\")\n",
    "#PARA QUE FUNCIONE EN 2023 EDITAR EL INFO DATA EN donde pone Sample number cambiar los 0 por 1.\n",
    "info_data_completed.loc[info_data_completed['Sample_number'] == 0, 'Sample_number'] = 1\n",
    "\n",
    "\n",
    "ungroup_pic(input_contours=contours_groups, output_path=working_directory, info_file=info_data_completed, axis=\"Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shell Almonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a entrenar un modelo para las almendras con cascara\n",
    "#Vamos a separar en una carpeta las imagenes de almendras con cascara\n",
    "info_data_df=pd.read_csv(f\"{working_directory}/info_data_completed_ungrouped.txt\", sep=\"\\t\")\n",
    "info_data_df = info_data_df[info_data_df[\"Shell\"] == \"Yes\"]\n",
    "\n",
    "# Directorio donde están las imágenes\n",
    "directorio_imagenes = f\"{working_directory}/Ungrouped_pics\"  # Cambia esto por la ruta correcta\n",
    "# Directorio de destino donde copiarás las imágenes\n",
    "directorio_destino = f\"{working_directory}/Shell_ungrouped_pics\"\n",
    "\n",
    "# Crear el directorio de destino si no existe\n",
    "os.makedirs(directorio_destino, exist_ok=True)\n",
    "\n",
    "# Copiar las imágenes\n",
    "for index, row in info_data_df.iterrows():\n",
    "    imagen_relativa = row['Sample_picture']  # Obtener la ruta relativa de la imagen\n",
    "    imagen_path = os.path.join(directorio_imagenes, imagen_relativa)  # Ruta completa\n",
    "\n",
    "    try:\n",
    "        # Copiar la imagen al directorio de destino\n",
    "        shutil.copy(imagen_path, os.path.join(directorio_destino, os.path.basename(imagen_path)))\n",
    "        print(f\"Imagen copiada: {imagen_path} a {directorio_destino}\")\n",
    "    except Exception as e:\n",
    "        print(f\"No se pudo copiar la imagen {imagen_path}. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bien ahora vamos a hacer el slicing y a segmentar y etiquetar las imagenes\n",
    "\n",
    "directorio_imagenes=os.path.join(working_directory, \"Ungrouped_pics_shell_2023\")\n",
    "\n",
    "\n",
    "slicing(input_folder=directorio_imagenes,output_directory=working_directory,name_slicing=\"Slices_for_shell_2023_29102024_320\", number_pictures=50, slice_height=320, slice_width=320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora vamos a entrenar el modelo \n",
    "\n",
    "shell_model=model_segmentation(working_directory=working_directory)\n",
    "shell_model.train_segmentation_model(input_zip=zip_file_shell, epochs=100,imgsz=640, name_segmentation=\"shell_2022_yolov11\",\n",
    "                                      pre_model=pre_model, batch=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora podemos utilizar ya el modelo y la clase picture para obtener todas las medidas\n",
    "#Con watershed\n",
    "\n",
    "info_data_df=pd.read_csv(f\"{working_directory}/info_data_completed_ungrouped_shell2023.txt\", sep=\"\\t\")\n",
    "info_data_df = info_data_df[info_data_df[\"Shell\"] == \"Yes\"]\n",
    "input_folder=os.path.join(working_directory, \"fotos_prueba_2023\")\n",
    "shell_masks=model_segmentation(working_directory=working_directory)\n",
    "shell_masks=shell_masks.slice_predict_reconstruct(input_folder=input_folder,imgsz=320, model_path=shell_model_path,\n",
    "                                          slice_height=320, slice_width=320,overlap_height_ratio=0.2,\n",
    "                                          overlap_width_ratio=0.2)\n",
    "shell_object=pictures(working_directory=working_directory, input_folder=input_folder,info_file=info_data_df,\n",
    "                      fruit=\"Shell_almond\", binary_masks=False, project_name=\"Shell_2023_prueba_30102024\")\n",
    "shell_object.set_postsegmentation_parameters(sahi=False, segmentation_input=shell_masks, smoothing=True, smoothing_iterations=2, kernel_smoothing=3,\n",
    "                        watershed=True, kernel_watershed=5, threshold_watershed=0.6)\n",
    "shell_object.measure_almonds(margin=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU detectada: NVIDIA GeForce RTX 3060\n",
      "Memoria total de la GPU: 12.00 GB\n",
      "Pic 1/13\n",
      "Performing prediction on 21 slices.\n",
      "Pic 2/13\n",
      "Performing prediction on 21 slices.\n",
      "Pic 3/13\n",
      "Performing prediction on 21 slices.\n",
      "Pic 4/13\n",
      "Performing prediction on 21 slices.\n",
      "Pic 5/13\n",
      "Performing prediction on 21 slices.\n",
      "Pic 6/13\n",
      "Performing prediction on 21 slices.\n",
      "Pic 7/13\n",
      "Performing prediction on 21 slices.\n",
      "Pic 8/13\n",
      "Performing prediction on 21 slices.\n",
      "Pic 9/13\n",
      "Performing prediction on 21 slices.\n",
      "Pic 10/13\n",
      "Performing prediction on 21 slices.\n",
      "Pic 11/13\n",
      "Performing prediction on 21 slices.\n",
      "Pic 12/13\n",
      "Performing prediction on 21 slices.\n",
      "Pic 13/13\n",
      "Performing prediction on 21 slices.\n",
      "[        9.7] [     13.431] [     15.918]\n",
      "[     10.695] [     14.426] [     16.664]\n",
      "[     9.2026] [     13.679] [     15.669]\n",
      "[     9.9487] [     13.679] [     16.415]\n",
      "[        9.7] [     12.933] [     15.918]\n",
      "[     9.4513] [     13.928] [     16.167]\n",
      "[     3.7308] [     9.2026] [     13.182]\n",
      "[     11.192] [     15.172] [     16.913]\n",
      "[     10.695] [     14.177] [     16.664]\n",
      "[     10.695] [     13.928] [     16.167]\n",
      "[     7.2128] [     10.944] [     12.187]\n",
      "[     8.9538] [     10.695] [     12.436]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pheno\\OneDrive - UNIVERSIDAD DE MURCIA\\Escritorio\\Almond_CV\\almondcv2\\pictures_class.py:423: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  morphology_table = pd.concat([morphology_table, row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[      7.959] [     11.441] [     14.177]\n",
      "[     9.9487] [     13.679] [     15.421]\n",
      "[     9.4513] [     12.933] [     14.923]\n",
      "[     10.944] [     13.431] [     15.421]\n",
      "[     9.2026] [     13.182] [     14.923]\n",
      "[     8.7051] [     12.187] [     14.177]\n",
      "[     9.9487] [     12.436] [     14.426]\n",
      "[     9.9487] [     12.187] [     13.431]\n",
      "[     9.2026] [     13.431] [     15.918]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pheno\\OneDrive - UNIVERSIDAD DE MURCIA\\Escritorio\\Almond_CV\\almondcv2\\pictures_class.py:439: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  general_table=pd.concat([general_table,row_general], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     12.982] [     16.167] [     19.106]\n",
      "[     8.8182] [     15.187] [     19.351]\n",
      "[     12.003] [     17.391] [     20.576]\n",
      "[      9.553] [     13.472] [     16.167]\n",
      "[     11.513] [     17.881] [     21.311]\n",
      "[     14.452] [     19.106] [     21.311]\n",
      "[     11.758] [     16.657] [     20.331]\n",
      "[     10.288] [     15.187] [     19.106]\n",
      "[     12.003] [     17.636] [     20.086]\n",
      "[     12.982] [     18.371] [     21.556]\n",
      "[     12.492] [     16.167] [     19.596]\n",
      "[     8.8182] [     14.207] [     17.391]\n",
      "[     10.043] [     13.717] [     16.657]\n",
      "[     9.3081] [     16.167] [     20.331]\n",
      "[     12.247] [     17.146] [     20.576]\n",
      "[     11.023] [     15.922] [     18.616]\n",
      "[     10.778] [     16.902] [     21.066]\n",
      "[     9.3081] [     13.227] [     16.167]\n",
      "[      9.798] [     15.187] [     18.861]\n",
      "[     12.982] [     15.922] [     18.616]\n",
      "[     10.778] [     16.412] [     19.351]\n",
      "[     10.393] [     12.867] [     14.352]\n",
      "[     10.393] [     13.362] [     15.589]\n",
      "[     10.888] [     14.352] [     16.332]\n",
      "[     9.4031] [     12.372] [     14.599]\n",
      "[     8.4133] [     12.867] [     14.847]\n",
      "[      11.63] [     15.094] [     16.827]\n",
      "[     10.888] [     14.847] [     16.827]\n",
      "[     8.4133] [     11.878] [     12.867]\n",
      "[     9.6505] [     13.115] [     16.827]\n",
      "[      10.64] [     15.342] [     18.064]\n",
      "[     11.878] [     15.094] [     17.321]\n",
      "[     10.145] [     15.094] [     17.321]\n",
      "[     8.9082] [     11.135] [     14.105]\n",
      "[     11.383] [     14.352] [     17.816]\n",
      "[     8.4133] [      12.62] [     14.599]\n",
      "[     10.888] [     13.857] [     16.084]\n",
      "[     8.9082] [     12.125] [     14.352]\n",
      "[      11.63] [     14.352] [     16.084]\n",
      "[     7.6709] [      10.64] [     12.372]\n",
      "[     10.888] [     14.105] [     15.342]\n",
      "[     6.1862] [     9.1556] [     10.393]\n",
      "[     11.325] [     14.279] [     16.495]\n",
      "[     10.094] [     14.033] [     16.003]\n",
      "[     7.8782] [     12.063] [     14.525]\n",
      "[     11.325] [     14.033] [     16.003]\n",
      "[     8.1244] [     11.571] [     13.294]\n",
      "[      10.34] [     13.294] [     15.264]\n",
      "[     10.586] [     14.772] [     17.234]\n",
      "[     12.802] [     15.756] [     19.695]\n",
      "[     8.8629] [     12.063] [     14.525]\n",
      "[      10.34] [     13.294] [      15.51]\n",
      "[      10.34] [      12.31] [     14.279]\n",
      "[     12.802] [     15.264] [      17.48]\n",
      "[     7.8782] [     12.802] [     15.264]\n",
      "[     8.8629] [     11.817] [     15.018]\n",
      "[      10.34] [     13.541] [     16.495]\n",
      "[     10.586] [     14.279] [     17.234]\n",
      "[     10.586] [     13.787] [     16.003]\n",
      "[     13.048] [     15.756] [      17.48]\n",
      "[     5.1701] [     9.3553] [      12.31]\n",
      "[     7.8782] [     10.586] [     12.802]\n",
      "[     11.571] [     14.525] [     16.495]\n",
      "[     12.436] [     16.415] [     19.151]\n",
      "[     14.923] [     18.156] [     21.638]\n",
      "[     14.923] [     17.908] [     19.649]\n",
      "[     12.436] [     16.913] [     18.654]\n",
      "[     13.679] [     17.659] [     19.649]\n",
      "[     11.441] [     15.918] [     18.405]\n",
      "[     7.4615] [      11.69] [     13.679]\n",
      "[     12.187] [     15.172] [     17.162]\n",
      "[     8.9538] [     12.685] [     14.674]\n",
      "[     12.933] [     16.664] [     19.151]\n",
      "[     10.944] [     15.918] [     18.903]\n",
      "[     10.944] [     14.923] [      17.41]\n",
      "[     10.446] [     14.177] [     15.669]\n",
      "[     15.421] [     17.908] [     20.395]\n",
      "[     13.431] [     17.659] [     19.649]\n",
      "[     13.431] [     16.415] [     18.405]\n",
      "[     11.938] [     14.426] [     16.415]\n",
      "[     9.9487] [     13.431] [     14.923]\n",
      "[     8.7051] [     14.426] [     16.664]\n",
      "[     13.182] [     15.918] [     18.654]\n",
      "[     13.431] [     18.156] [     20.644]\n",
      "[     13.857] [     18.806] [     21.281]\n",
      "[      12.62] [     16.084] [     18.311]\n",
      "[     12.867] [     16.332] [     18.311]\n",
      "[     9.4031] [     14.105] [     17.569]\n",
      "[     9.6505] [     13.857] [     15.589]\n",
      "[     10.145] [     16.579] [     19.548]\n",
      "[     9.4031] [     15.342] [     20.786]\n",
      "[      13.61] [     18.806] [     21.528]\n",
      "[     10.393] [     15.094] [     17.816]\n",
      "[     11.383] [     16.827] [     18.559]\n",
      "[      9.898] [     15.342] [     18.806]\n",
      "[     12.372] [     16.579] [     19.301]\n",
      "[     12.125] [     18.806] [     22.518]\n",
      "[     8.6607] [     12.125] [     14.105]\n",
      "[     7.6709] [     11.383] [     13.857]\n",
      "[     11.441] [     16.664] [       19.4]\n",
      "[     12.187] [     16.415] [     19.649]\n",
      "[     9.9487] [     15.669] [     18.654]\n",
      "[     12.436] [     16.415] [     19.151]\n",
      "[     12.187] [     15.918] [     18.903]\n",
      "[     9.4513] [     13.928] [     16.664]\n",
      "[     11.938] [     16.664] [     18.654]\n",
      "[        9.7] [     14.426] [     17.659]\n",
      "[     8.7051] [     13.679] [     16.415]\n",
      "[     9.9487] [     14.177] [     17.162]\n",
      "[     10.446] [     15.172] [      17.41]\n",
      "[     10.944] [     15.421] [     17.659]\n",
      "[     10.446] [     14.674] [     18.654]\n",
      "[     11.938] [     16.167] [     18.654]\n",
      "[      11.69] [     16.664] [       19.4]\n",
      "[     7.4615] [        9.7] [     10.944]\n",
      "[     5.9692] [      11.69] [     14.177]\n",
      "[     11.938] [     16.913] [     20.644]\n",
      "[     11.938] [     16.167] [     18.405]\n",
      "[        9.7] [     12.685] [     15.669]\n",
      "[     10.695] [     13.182] [     15.421]\n",
      "[     10.043] [     14.697] [     17.881]\n",
      "[     10.288] [     15.187] [     16.902]\n",
      "[     11.023] [     14.697] [     17.881]\n",
      "[     11.268] [     15.187] [     17.146]\n",
      "[     14.452] [     18.126] [     20.576]\n",
      "[     11.758] [     15.677] [     17.391]\n",
      "[     11.758] [     16.412] [     17.391]\n",
      "[     9.3081] [     13.472] [     14.942]\n",
      "[     11.513] [     15.432] [     18.371]\n",
      "[     11.023] [     14.942] [     17.146]\n",
      "[     11.758] [     16.167] [     18.861]\n",
      "[     11.268] [     15.432] [     17.636]\n",
      "[     10.043] [     14.697] [     17.146]\n",
      "[     12.982] [     17.636] [     19.596]\n",
      "[     7.5934] [     11.758] [     14.452]\n",
      "[     12.003] [     15.677] [     17.881]\n",
      "[     10.533] [     14.942] [     17.636]\n",
      "[     8.8182] [     14.452] [     16.657]\n",
      "[     10.533] [     13.717] [     16.657]\n",
      "[     10.778] [     13.717] [     15.677]\n",
      "[     9.3081] [     12.982] [     15.187]\n",
      "[      10.64] [     14.352] [     17.321]\n",
      "[     10.145] [      13.61] [     16.084]\n",
      "[     10.888] [     14.847] [     17.569]\n",
      "[     8.9082] [      12.62] [     14.352]\n",
      "[     7.9184] [      13.61] [     17.321]\n",
      "[      10.64] [     15.837] [     18.311]\n",
      "[     10.393] [     14.352] [     16.084]\n",
      "[     8.1658] [     12.867] [     15.589]\n",
      "[      11.63] [     14.847] [     17.569]\n",
      "[     13.362] [     17.074] [     18.806]\n",
      "[     10.145] [     13.362] [     14.599]\n",
      "[     8.4133] [     12.867] [     15.589]\n",
      "[     6.9286] [     9.4031] [     12.125]\n",
      "[     11.135] [     14.599] [     17.074]\n",
      "[      7.176] [     12.372] [     14.847]\n",
      "[     9.6505] [     12.125] [     14.105]\n",
      "[     8.1658] [     12.372] [     14.599]\n",
      "[     7.6709] [     12.125] [     14.847]\n",
      "[     10.393] [     14.599] [     16.332]\n",
      "[      9.898] [     13.362] [     15.837]\n",
      "[     8.4133] [     12.125] [     14.599]\n",
      "[     9.4575] [      12.61] [     14.065]\n",
      "[     5.5775] [      10.67] [      13.58]\n",
      "[      5.335] [     11.398] [     13.338]\n",
      "[       8.73] [     12.125] [      15.52]\n",
      "[      10.67] [     14.308] [     16.975]\n",
      "[        9.7] [     13.338] [     16.005]\n",
      "[     7.5175] [     10.428] [      12.61]\n",
      "[       6.79] [     10.913] [     13.095]\n",
      "[     8.4875] [     11.155] [     13.338]\n",
      "[       8.73] [     12.853] [      15.52]\n",
      "[     7.0325] [        9.7] [     11.883]\n",
      "[     8.9725] [     12.853] [      16.49]\n",
      "[     9.4575] [     11.883] [     13.095]\n",
      "[     8.4875] [     12.125] [     14.793]\n",
      "[        9.7] [     13.095] [     15.763]\n",
      "[      8.245] [     11.883] [     14.793]\n",
      "[        9.7] [     13.338] [     15.763]\n",
      "[     8.9725] [     12.368] [      14.55]\n",
      "[     11.155] [     14.065] [     16.975]\n",
      "[       8.73] [     11.883] [     14.065]\n",
      "[       6.79] [     9.9425] [     11.883]\n",
      "[     9.0466] [     13.067] [     17.088]\n",
      "[     9.0466] [     14.826] [     18.345]\n",
      "[     11.057] [     15.832] [     19.601]\n",
      "[     9.8005] [     12.816] [     15.329]\n",
      "[      8.544] [      11.56] [     13.821]\n",
      "[     7.0363] [      11.56] [     14.826]\n",
      "[     10.303] [     14.073] [     16.334]\n",
      "[     12.313] [     16.083] [     18.596]\n",
      "[      11.56] [     15.832] [     18.847]\n",
      "[     9.5492] [     15.078] [     17.591]\n",
      "[      13.57] [     16.837] [      19.35]\n",
      "[     10.806] [      15.58] [     18.596]\n",
      "[     13.067] [     17.088] [     19.601]\n",
      "[     10.806] [     14.073] [     17.088]\n",
      "[     10.303] [     14.575] [     17.088]\n",
      "[     9.8005] [     15.078] [     18.093]\n",
      "[      8.544] [     14.575] [     17.088]\n",
      "[      8.544] [     12.565] [     14.575]\n",
      "[     6.0311] [     14.073] [     17.339]\n",
      "[     6.0311] [      11.56] [     15.832]\n",
      "[     4.5233] [     12.062] [     14.324]\n",
      "[     15.589] [     19.548] [     21.281]\n",
      "[     13.857] [     19.054] [     22.765]\n",
      "[     15.094] [     19.301] [     25.487]\n",
      "[     15.342] [     20.538] [     25.735]\n",
      "[     13.857] [     18.064] [     21.528]\n",
      "[     13.115] [     19.301] [     23.755]\n",
      "[     14.847] [     20.786] [      24.25]\n",
      "[     14.105] [     19.548] [      23.26]\n",
      "[     15.589] [     21.033] [      25.24]\n",
      "[     12.125] [     16.827] [     20.786]\n",
      "[     11.135] [     17.321] [     21.528]\n",
      "[     14.847] [     18.559] [      23.26]\n",
      "[     11.878] [     16.084] [     18.806]\n",
      "[     12.867] [     17.321] [     20.043]\n",
      "[      12.62] [     18.559] [      22.27]\n",
      "[     15.094] [     20.291] [      24.25]\n",
      "[     12.867] [     18.559] [     22.518]\n",
      "[     14.352] [     18.806] [      22.27]\n",
      "[     13.362] [     19.301] [     22.518]\n",
      "[     14.105] [     19.796] [     24.003]\n",
      "[      13.61] [     18.311] [     21.528]\n",
      "[     14.136] [     18.766] [     21.447]\n",
      "[     9.7487] [     15.354] [     19.254]\n",
      "[     12.186] [     16.817] [     20.716]\n",
      "[     9.7487] [     15.842] [     19.254]\n",
      "[     10.236] [     16.329] [     19.497]\n",
      "[     14.623] [     19.497] [     21.691]\n",
      "[     13.161] [     17.304] [     19.254]\n",
      "[     10.724] [     15.111] [     18.279]\n",
      "[      7.799] [     14.623] [     18.035]\n",
      "[     11.942] [     16.817] [     20.716]\n",
      "[     11.942] [     17.304] [     18.766]\n",
      "[      10.48] [     16.329] [     18.035]\n",
      "[     10.236] [     14.136] [      17.06]\n",
      "[     8.5302] [     16.817] [     21.204]\n",
      "[     9.2613] [     13.161] [     16.085]\n",
      "[     13.648] [     17.304] [     19.741]\n",
      "[      9.505] [     15.354] [     17.548]\n",
      "[     9.9925] [     14.379] [     16.329]\n",
      "[     9.0176] [     12.673] [     14.136]\n",
      "[     10.236] [     14.867] [     16.573]\n",
      "[      10.48] [     14.623] [      17.06]\n"
     ]
    }
   ],
   "source": [
    "#Ahora podemos utilizar ya el modelo y la clase picture para obtener todas las medidas\n",
    "#Con sahi\n",
    "\n",
    "info_data_df=pd.read_csv(f\"{working_directory}/info_data_completed_ungrouped_shell2023.txt\", sep=\"\\t\")\n",
    "info_data_df = info_data_df[info_data_df[\"Shell\"] == \"Yes\"]\n",
    "input_folder=os.path.join(working_directory, \"fotos_prueba_2023\")\n",
    "shell_masks=model_segmentation(working_directory=working_directory)\n",
    "shell_masks=shell_masks.predict_model_sahi(model_path=shell_model_path, check_result=False, folder_input=input_folder,\n",
    "                                            retina_masks=True,\n",
    "                                              postprocess_match_threshold=0.2, overlap_height_ratio=0.3,\n",
    "                                                overlap_width_ratio=0.3, postprocess_match_metric=\"IOS\", \n",
    "                                                postprocess_type=\"GREEDYNMM\", slice_height=320, slice_width=320,\n",
    "                                                  confidence_treshold=0.95,\n",
    "                                                  imgsz=320)\n",
    "shell_object=pictures(working_directory=working_directory, input_folder=input_folder,info_file=info_data_df,\n",
    "                      fruit=\"Shell_almond\", binary_masks=True, project_name=\"Shell_2023_prueba_sahi_30102024\")\n",
    "shell_object.set_postsegmentation_parameters(sahi=True, segmentation_input=shell_masks)\n",
    "shell_object.measure_almonds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seed Almonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vamos a entrenar un modelo para las almendras sin cascara\n",
    "#Vamos a separar en una carpeta las imagenes de almendras sin cascara\n",
    "info_data_df=pd.read_csv(f\"{working_directory}/info_data_completed_ungrouped.txt\", sep=\"\\t\")\n",
    "info_data_df = info_data_df[info_data_df[\"Shell\"] == \"No\"]\n",
    "\n",
    "# Directorio donde están las imágenes\n",
    "directorio_imagenes = f\"{working_directory}/Ungrouped_pics\"  # Cambia esto por la ruta correcta\n",
    "# Directorio de destino donde copiarás las imágenes\n",
    "directorio_destino = f\"{working_directory}/Seed_ungrouped_pics\"\n",
    "\n",
    "# Crear el directorio de destino si no existe\n",
    "os.makedirs(directorio_destino, exist_ok=True)\n",
    "\n",
    "# Copiar las imágenes\n",
    "for index, row in info_data_df.iterrows():\n",
    "    imagen_relativa = row['Sample_picture']  # Obtener la ruta relativa de la imagen\n",
    "    imagen_path = os.path.join(directorio_imagenes, imagen_relativa)  # Ruta completa\n",
    "\n",
    "    try:\n",
    "        # Copiar la imagen al directorio de destino\n",
    "        shutil.copy(imagen_path, os.path.join(directorio_destino, os.path.basename(imagen_path)))\n",
    "        print(f\"Imagen copiada: {imagen_path} a {directorio_destino}\")\n",
    "    except Exception as e:\n",
    "        print(f\"No se pudo copiar la imagen {imagen_path}. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slicing(input_folder=directorio_destino,output_directory=working_directory,name_slicing=\"Slices_for_seed_21102024_320\", number_pictures=30, slice_height=320, slice_width=320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_model=model_segmentation(working_directory=working_directory)\n",
    "seed_model.train_segmentation_model(input_zip=zip_file_seed, epochs=100,imgsz=640, name_segmentation=\"seed_2022_211024\",\n",
    "                                      pre_model=pre_model, batch=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora podemos utilizar ya el modelo y la clase picture para obtener todas las medidas\n",
    "\n",
    "info_data_df=pd.read_csv(f\"{working_directory}/info_data_completed_ungrouped.txt\", sep=\"\\t\")\n",
    "info_data_df = info_data_df[info_data_df[\"Shell\"] == \"No\"]\n",
    "input_folder=os.path.join(working_directory, \"fotos_prueba\")\n",
    "seed_masks=model_segmentation(working_directory=working_directory)\n",
    "seed_masks=shell_masks.slice_predict_reconstruct(input_folder=input_folder,imgsz=640, model_path=shell_model_path,\n",
    "                                          slice_height=320, slice_width=320,overlap_height_ratio=0.2,\n",
    "                                          overlap_width_ratio=0.2)\n",
    "seed_object=pictures(working_directory=working_directory, input_folder=input_folder,info_file=info_data_df,\n",
    "                      fruit=\"Seed_almond\", binary_masks=True, project_name=\"Seed_2022_21102022\",\n",
    "                        segmentation_maks=shell_masks, smoothing=True, smoothing_iterations=2, kernel_smoothing=5,\n",
    "                        watershed=True, kernel_watershed=5, threshold_watershed=0.5)\n",
    "seed_object.measure_almonds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUCIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROTAR \n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Ruta de la carpeta con las imágenes\n",
    "folder_path = output_calibrated\n",
    "\n",
    "# Recorrer cada archivo en la carpeta\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):  # Filtrar solo archivos de imagen\n",
    "        # Ruta completa del archivo\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Abrir la imagen\n",
    "        with Image.open(img_path) as img:\n",
    "            # Rotar la imagen 90 grados en sentido contrario a las agujas del reloj\n",
    "            rotated_img = img.rotate(90, expand=True)\n",
    "            \n",
    "            # Sobrescribir la imagen rotada en la misma ubicación\n",
    "            rotated_img.save(img_path)\n",
    "\n",
    "print(\"Rotación y guardado completados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quiero separar en dos carpetas shell y no shell\n",
    "\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Cargar el DataFrame (asegúrate de cambiar 'ruta_al_archivo' por la ruta de tu archivo)\n",
    "df = pd.read_csv(info_data,sep=\"\\t\")\n",
    "\n",
    "# Definir las carpetas de origen y destino\n",
    "carpeta_origen = output_calibrated\n",
    "carpeta_destino_yes = os.path.join(working_directory, \"Shell_2023_set\")\n",
    "carpeta_destino_no = os.path.join(working_directory, \"Seed_2023_set\")\n",
    "\n",
    "# Crear las carpetas de destino si no existen\n",
    "os.makedirs(carpeta_destino_yes, exist_ok=True)\n",
    "os.makedirs(carpeta_destino_no, exist_ok=True)\n",
    "\n",
    "# Iterar sobre las filas del DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    nombre_imagen = row['Name_picture']  # Cambia 'NombreImagen' al nombre de la columna en tu DataFrame\n",
    "    shell_value = row['Shell']           # Cambia 'Shell' al nombre exacto de la columna en tu DataFrame\n",
    "    nombre_imagen = f\"CL_{nombre_imagen}\"\n",
    "    # Definir la ruta completa de la imagen en la carpeta de origen\n",
    "    ruta_origen = os.path.join(carpeta_origen, nombre_imagen)\n",
    "    \n",
    "    # Comprobar si el valor de Shell es 'Yes' o 'No' y definir la carpeta de destino\n",
    "    if shell_value == 'Yes':\n",
    "        ruta_destino = os.path.join(carpeta_destino_yes, nombre_imagen)\n",
    "    elif shell_value == 'No':\n",
    "        ruta_destino = os.path.join(carpeta_destino_no, nombre_imagen)\n",
    "    else:\n",
    "        continue  # Saltar filas con otros valores en 'Shell'\n",
    "    \n",
    "    # Copiar la imagen a la carpeta de destino correspondiente\n",
    "    try:\n",
    "        shutil.copy(ruta_origen, ruta_destino)\n",
    "        print(f\"Imagen '{nombre_imagen}' copiada a {ruta_destino}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Imagen '{nombre_imagen}' no encontrada en {ruta_origen}\")\n",
    "\n",
    "print(\"Copia de imágenes completada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora podemos utilizar ya el modelo y la clase picture para obtener todas las medidas\n",
    "\n",
    "info_data_df=pd.read_csv(f\"{working_directory}/info_data_completed_ungrouped.txt\", sep=\"\\t\")\n",
    "info_data_df = info_data_df[info_data_df[\"Shell\"] == \"Yes\"]\n",
    "input_folder=os.path.join(working_directory, \"fotos_prueba\")\n",
    "shell_model_path=os.path.join(models_directory, \"prueba_Retina_mask_640_shell.pt\")\n",
    "shell_masks=model_segmentation(working_directory=working_directory)\n",
    "shell_masks=shell_masks.slice_predict_reconstruct(input_folder=input_folder,imgsz=640, model_path=shell_model_path,\n",
    "                                          slice_height=640, slice_width=640,overlap_height_ratio=0.2,\n",
    "                                          overlap_width_ratio=0.2, retina_mask=True)\n",
    "shell_object=pictures(working_directory=working_directory, input_folder=input_folder,info_file=info_data_df,\n",
    "                      fruit=\"Shell_almond\", binary_masks=True, project_name=\"pruebas_retina_mask\",\n",
    "                        segmentation_maks=shell_masks, smoothing=False, smoothing_iterations=2, kernel_smoothing=5,\n",
    "                        watershed=False, kernel_watershed=5, threshold_watershed=0.5)\n",
    "shell_object.measure_almonds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_data_df=pd.read_csv(f\"{working_directory}/info_data_completed_ungrouped_2022.txt\", sep=\"\\t\")\n",
    "info_data_df = info_data_df[info_data_df[\"Shell\"] == \"Yes\"]\n",
    "input_folder=os.path.join(working_directory, \"fotos_prueba\")\n",
    "shell_model_path=os.path.join(models_directory, \"prueba_Retina_mask_320_shell.pt\")\n",
    "shell_masks=model_segmentation(working_directory=working_directory)\n",
    "shell_masks=shell_masks.predict_model_sahi(model_path=shell_model_path, check_result=True, folder_input=input_folder,\n",
    "                                            retina_masks=True,\n",
    "                                              postprocess_match_threshold=0.2, overlap_height_ratio=0.3,\n",
    "                                                overlap_width_ratio=0.3, postprocess_match_metric=\"IOS\", \n",
    "                                                postprocess_type=\"GREEDYNMM\", slice_height=320, slice_width=320,\n",
    "                                                  confidence_treshold=0.9,\n",
    "                                                  imgsz=320)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_data_df=pd.read_csv(f\"{working_directory}/info_data_completed_ungrouped_2022.txt\", sep=\"\\t\")\n",
    "info_data_df = info_data_df[info_data_df[\"Shell\"] == \"Yes\"]\n",
    "input_folder=os.path.join(working_directory, \"fotos_prueba\")\n",
    "shell_model_path=os.path.join(models_directory, \"prueba_Retina_mask_320_shell.pt\")\n",
    "shell_masks=model_segmentation(working_directory=working_directory)\n",
    "shell_masks=shell_masks.predict_model_sahi(model_path=shell_model_path, check_result=True, folder_input=input_folder,\n",
    "                                            retina_masks=True,\n",
    "                                              postprocess_match_threshold=0.3, overlap_height_ratio=0.2,\n",
    "                                                overlap_width_ratio=0.2, postprocess_match_metric=\"IOS\", \n",
    "                                                postprocess_type=\"GREEDYNMM\", slice_height=320, slice_width=320,\n",
    "                                                  confidence_treshold=0.9,\n",
    "                                                  imgsz=320)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids_df = info_data_completed[~info_data_completed['ID'].str.lower().duplicated(keep=False)]\n",
    "unique_ids_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = info_data_completed[info_data_completed['Shell'] == \"No\"]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEED MODEL 2023\n",
    "zip_file=os.path.join(working_directory,\"seed_320_2023_29102024.zip\")\n",
    "model=model_segmentation(working_directory=working_directory)\n",
    "model.train_segmentation_model(input_zip=zip_file, epochs=100,imgsz=320,\n",
    "                                name_segmentation=\"seed_2023_320_yolov11\",\n",
    "                                      pre_model=pre_model, batch=16, retina_masks=True)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shell MODEL 2023\n",
    "zip_file=os.path.join(working_directory,\"shell_2023_320.zip\")\n",
    "model=model_segmentation(working_directory=working_directory)\n",
    "model.train_segmentation_model(input_zip=zip_file, epochs=100,imgsz=320,\n",
    "                                name_segmentation=\"shell_2023_320_yolov11\",\n",
    "                                      pre_model=pre_model, batch=16, retina_masks=True)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seed MODEL 2022\n",
    "zip_file=os.path.join(working_directory,\"seed_2022_21102024.zip\")\n",
    "model=model_segmentation(working_directory=working_directory)\n",
    "model.train_segmentation_model(input_zip=zip_file, epochs=100,imgsz=320,\n",
    "                                name_segmentation=\"seed_2022_320_yolov11\",\n",
    "                                      pre_model=pre_model, batch=16, retina_masks=True)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shell MODEL 2022\n",
    "zip_file=os.path.join(working_directory,\"shell_2022_320.zip\")\n",
    "model=model_segmentation(working_directory=working_directory)\n",
    "model.train_segmentation_model(input_zip=zip_file, epochs=100,imgsz=320,\n",
    "                                name_segmentation=\"shell_2022_320_yolov11\",\n",
    "                                      pre_model=pre_model, batch=16, retina_masks=True)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coin model 2022\n",
    "zip_file=os.path.join(working_directory,\"coin_640_21102024.zip\")\n",
    "model=model_segmentation(working_directory=working_directory)\n",
    "model.train_segmentation_model(input_zip=zip_file, epochs=100,imgsz=640,\n",
    "                                name_segmentation=\"coin_2022_640_yolov11\",\n",
    "                                      pre_model=pre_model, batch=8, retina_masks=True)\n",
    "del model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
