{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORKFLOW COMPLETE FOR GOOGLE COLAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "repo_path = \"/content/drive/MyDrive/GitHub\"\n",
    "\n",
    "os.makedirs(repo_path, exist_ok=True)\n",
    "\n",
    "# Move to folder\n",
    "%cd {repo_path}\n",
    "\n",
    "# Clone\n",
    "!git clone https://github.com/jorgemasgomez/almondcv2.git\n",
    "\n",
    "%cd {repo_path}/almondcv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you have cloned the repository previously simply move to the folder \n",
    "repo_path = \"/content/drive/MyDrive/GitHub\"\n",
    "%cd {repo_path}/almondcv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements_google_colab.txt #In Google Colab will be necessary to install it in each session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pre-processing workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import os\n",
    "from calibrations import  build_calibration, calibrate_color_and_distortion, calibrate_color, calibrate_distortion\n",
    "from aux_functions import obtain_pixel_metric, ungroup_pic\n",
    "from model_class import ModelSegmentation\n",
    "import pandas as pd\n",
    "\n",
    "#Set paths of the files\n",
    "working_directory=\"/content/drive/MyDrive/database_almondcv2/\"\n",
    "chessboards=os.path.join(working_directory, \"calibracion/chessboards\") #folder with chessboard pitcures\n",
    "raw_folder=os.path.join(working_directory,\"pruebas_jorge\")#folder with the pictures to calibrate\n",
    "mtx_input_path=os.path.join(chessboards,\"calibration_mtx.npz\") #for distortion in npz format\n",
    "standard_matrix_color=os.path.join(working_directory, \"pruebas_jorge/28_10_CG-009.JPG\") #picture of reference\n",
    "output_calibrated=os.path.join(working_directory,\"pruebas_jorge\") #output folder for calibrated pictures\n",
    "\n",
    "coin_model_path=os.path.join(working_directory,\"models/coin_2022_yolov11_640.pt\")\n",
    "info_table=os.path.join(working_directory,\"info_data.txt\")\n",
    "\n",
    "group_model_path=os.path.join(working_directory, \"models/rectangle_2022_yolov11s_1280.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color and Distortion calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First build your distortion model based in chessboards\n",
    "build_calibration(chessboardSize=(6, 8), frameSize=(5472,3648), dir_path=chessboards, \n",
    "                  image_format=\".jpg\", size_of_chessboard_squares_mm=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for calibrate color and distortion\n",
    "calibrate_color_and_distortion(raw_folder=raw_folder,mtx_input_path=mtx_input_path,output_calibrated=output_calibrated,\n",
    "                                radius_param=10, standard_matrix=standard_matrix_color) #Standard matrix is a picture of reference to use instead of the original picture for error cases or simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for calibrate distortion only\n",
    "\n",
    "calibrate_distortion(input_folder=raw_folder, mtx_input=mtx_input_path, output_path=output_calibrated, input_picture=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for calibrate color only\n",
    "\n",
    "calibrate_color(input_folder=raw_folder, output_path=output_calibrated,standard_matrix=standard_matrix_color,\n",
    "                 force_standard_matrix=\"No\")  #force_standard_matrix option uses in all the pictures the reference picture. In negative case, use only standard_matrix in error cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain pixel size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deploy the model \n",
    "\n",
    "reference_model=ModelSegmentation(working_directory=working_directory)\n",
    "contours_coin=reference_model.slice_predict_reconstruct(input_folder=output_calibrated, imgsz=640,\n",
    "                                                         model_path=coin_model_path, slice_height=640, slice_width=640,\n",
    "                                                         overlap_height_ratio=0.1, overlap_width_ratio=0.1,\n",
    "                                                           retina_mask=True, conf=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load info table\n",
    "info_data_df=pd.read_csv(info_table,sep=\"\\t\")\n",
    "# If we use a calibrated dataset but the info table was previous we can include CL_ automatically with this line\n",
    "# info_data_df['Name_picture'] = info_data_df['Name_picture'].apply(lambda x: 'CL_' + x)\n",
    "\n",
    "info_data_completed=obtain_pixel_metric(info_data=info_data_df, contours=contours_coin,\n",
    "                                         output_directory=working_directory, reference=24.25) #reference in mm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ungroup pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load group model\n",
    "group_model=ModelSegmentation(working_directory=working_directory)\n",
    "contours_groups=group_model.predict_model(model_path=group_model_path,\n",
    "                               folder_input=output_calibrated,\n",
    "                               imgsz=1280, check_result=False, max_det=2, retina_mask=False) #Retina mask not recommended here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain sample pictures and update info table. \n",
    "info_data_completed_path=os.path.join(working_directory, \"info_data_completed_2022.txt\")\n",
    "info_data_completed=pd.read_csv(info_data_completed_path,sep=\"\\t\")\n",
    "\n",
    "ungroup_pic(input_contours=contours_groups, output_path=working_directory, info_file=info_data_completed, axis=\"X\") #axis indicate if the samples should be order according to Y or X axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Develop your segmentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import os\n",
    "from model_class import ModelSegmentation\n",
    "from aux_functions import slicing\n",
    "import cv2\n",
    "#Inputs\n",
    "working_directory=\"/content/drive/MyDrive/database_almondcv2/\"\n",
    "pictures_directory=os.path.join(working_directory, \"kernel_2022\")\n",
    "pre_model=os.path.join(working_directory, \"models/yolo11s-seg.pt\")\n",
    "output_directory=os.path.join(working_directory,\"output_directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slice_pictures for training\n",
    "slicing(input_folder=pictures_directory,output_directory=working_directory,name_slicing=\"Slicing_kernel_2022\", number_pictures=5, slice_height=320, slice_width=320)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label with CVAT\n",
    "\n",
    "zip_file=os.path.join(working_directory,\"kernel_2022_320.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model segmentation training\n",
    "\n",
    "model=ModelSegmentation(working_directory=working_directory)\n",
    "model.train_segmentation_model(input_zip=zip_file, epochs=50,imgsz=320, name_segmentation=\"kernel_2022_320\",\n",
    "                                      pre_model=pre_model, batch=16, colab=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy and reconstruct a picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slice_predict_reconstruct approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join patches approach\n",
    "\n",
    "model=ModelSegmentation(working_directory=working_directory)\n",
    "masks=model.slice_predict_reconstruct(input_folder=pictures_directory,imgsz=320, model_path=model_path,\n",
    "                                          slice_height=320, slice_width=320,overlap_height_ratio=0.2,\n",
    "                                          overlap_width_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show the masks\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "for mask in masks:\n",
    "    cv2.imwrite(f\"{output_directory}/{os.path.basename(mask[1])}\", mask[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ModelSegmentation(working_directory=working_directory)\n",
    "masks=model.predict_model_sahi(model_path=model_path, check_result=False, folder_input=pictures_directory,\n",
    "                                            retina_masks=True,\n",
    "                                              postprocess_match_threshold=0.2, overlap_height_ratio=0.2,\n",
    "                                                overlap_width_ratio=0.2, postprocess_match_metric=\"IOS\", \n",
    "                                                postprocess_type=\"GREEDYNMM\", slice_height=320, slice_width=320,\n",
    "                                                  confidence_treshold=0.8,\n",
    "                                                  imgsz=320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show the masks\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "for mask in masks:\n",
    "    mask[0].export_visuals(export_dir=output_directory, hide_labels=True, rect_th=1, file_name=f\"{os.path.basename(mask[1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deploy your segmentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import os\n",
    "from model_class import ModelSegmentation\n",
    "import pickle\n",
    "from pictures_class import Pictures\n",
    "import pandas as pd\n",
    "#Inputs\n",
    "working_directory=\"/content/drive/MyDrive/database_almondcv2/\"\n",
    "pictures_directory=os.path.join(working_directory, \"kernel_2022\")\n",
    "model_path=os.path.join(working_directory, \"models/kernel_2022_yolov11s_320.pt\")\n",
    "info_data_completed_path=os.path.join(working_directory, \"info_data_2022.txt\")\n",
    "info_data_completed_path_almond=os.path.join(working_directory, \"info_data_2022.txt\")\n",
    "info_data_completed=pd.read_csv(info_data_completed_path,sep=\"\\t\")\n",
    "info_data_completed_almond=pd.read_csv(info_data_completed_path_almond,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose your reconstruction approach and measure (almond)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice predict reconstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join patches approach\n",
    "\n",
    "model=ModelSegmentation(working_directory=working_directory)\n",
    "masks=model.slice_predict_reconstruct(input_folder=pictures_directory,imgsz=320, model_path=model_path,\n",
    "                                          slice_height=320, slice_width=320,overlap_height_ratio=0.2,\n",
    "                                          overlap_width_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example with slice predict reconstruct approach\n",
    "pictures_object=Pictures(working_directory=working_directory, input_folder=pictures_directory,info_file=info_data_completed_almond,\n",
    "                      fruit=\"kernel_almond\", binary_masks=True, project_name=\"probando_watershed\", blurring_binary_masks=False)\n",
    "pictures_object.set_postsegmentation_parameters(sahi=False, segmentation_input=masks, smoothing=False, smoothing_iterations=2, kernel_smoothing=3,\n",
    "                        watershed=True, kernel_watershed=5, threshold_watershed=0.6)\n",
    "pictures_object.measure_almonds(margin=400)\n",
    "\n",
    "# Save\n",
    "with open(f'{working_directory}/pictures_object_watershed.pkl', 'wb') as file:\n",
    "    pickle.dump(pictures_object, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ModelSegmentation(working_directory=working_directory)\n",
    "masks=model.predict_model_sahi(model_path=model_path, check_result=False, folder_input=pictures_directory,\n",
    "                                            retina_masks=True,\n",
    "                                              postprocess_match_threshold=0.2, overlap_height_ratio=0.2,\n",
    "                                                overlap_width_ratio=0.2, postprocess_match_metric=\"IOS\", \n",
    "                                                postprocess_type=\"GREEDYNMM\", slice_height=320, slice_width=320,\n",
    "                                                  confidence_treshold=0.8,\n",
    "                                                  imgsz=320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example with SAHI approach\n",
    "pictures_object=Pictures(working_directory=working_directory, input_folder=pictures_directory,info_file=info_data_completed_almond,\n",
    "                      fruit=\"kernel_almond\", binary_masks=True, project_name=\"probando\",  blurring_binary_masks=False)\n",
    "pictures_object.set_postsegmentation_parameters(sahi=True, segmentation_input=masks)\n",
    "pictures_object.measure_almonds(margin=400)\n",
    "\n",
    "# Guardar el objeto en un archivo\n",
    "with open(f'{working_directory}/pictures_object_sahi.pkl', 'wb') as file:\n",
    "    pickle.dump(pictures_object, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose your reconstruction approach and measure (general)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice predict reconstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join patches approach\n",
    "\n",
    "model=ModelSegmentation(working_directory=working_directory)\n",
    "masks=model.slice_predict_reconstruct(input_folder=pictures_directory,imgsz=320, model_path=model_path,\n",
    "                                          slice_height=320, slice_width=320,overlap_height_ratio=0.2,\n",
    "                                          overlap_width_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example with slice predict reconstruct approach\n",
    "pictures_object=Pictures(working_directory=working_directory, input_folder=pictures_directory,info_file=info_data_completed,\n",
    "                      fruit=\"kernel_almond\", binary_masks=True, project_name=\"kernel_watershed\", blurring_binary_masks=False)\n",
    "pictures_object.set_postsegmentation_parameters(sahi=False, segmentation_input=masks, smoothing=False, smoothing_iterations=2, kernel_smoothing=3,\n",
    "                        watershed=True, kernel_watershed=5, threshold_watershed=0.6)\n",
    "pictures_object.measure_general(margin=400)\n",
    "\n",
    "# Save\n",
    "with open(f'{working_directory}/pictures_object_watershed.pkl', 'wb') as file:\n",
    "    pickle.dump(pictures_object, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ModelSegmentation(working_directory=working_directory)\n",
    "masks=model.predict_model_sahi(model_path=model_path, check_result=False, folder_input=pictures_directory,\n",
    "                                            retina_masks=True,\n",
    "                                              postprocess_match_threshold=0.2, overlap_height_ratio=0.2,\n",
    "                                                overlap_width_ratio=0.2, postprocess_match_metric=\"IOS\", \n",
    "                                                postprocess_type=\"GREEDYNMM\", slice_height=320, slice_width=320,\n",
    "                                                  confidence_treshold=0.8,\n",
    "                                                  imgsz=320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example with SAHI approach\n",
    "pictures_object=Pictures(working_directory=working_directory, input_folder=pictures_directory,info_file=info_data_completed,\n",
    "                      fruit=\"kernel_almond\", binary_masks=True, project_name=\"kernel_2022_sahi\",  blurring_binary_masks=False)\n",
    "pictures_object.set_postsegmentation_parameters(sahi=True, segmentation_input=masks)\n",
    "pictures_object.measure_general(margin=400)\n",
    "\n",
    "# Guardar el objeto en un archivo\n",
    "with open(f'{working_directory}/pictures_object_sahi.pkl', 'wb') as file:\n",
    "    pickle.dump(pictures_object, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Morphometrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import os\n",
    "from morphometrics_functions import install_morphometrics_packages_r, exploratory_morphometrics_r, run_efourier_pca_morphometrics_r, run_plot_pca_morphometrics_r, run_kmeans_efourier_r, process_images_and_perform_pca\n",
    "\n",
    "#Inputs\n",
    "\n",
    "input_masks=r\"\"\n",
    "working_directory=r\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For installing libraries\n",
    "install_morphometrics_packages_r()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For exploring the dataset\n",
    "exploratory_morphometrics_r(info_data=\"\", grouping_factor=\"\", input_directory=input_masks,\n",
    "                             output_directory=working_directory, show=True, nharmonics=10,nexamples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For running EFA and PCA \n",
    "object_path=os.path.join(working_directory,\"exploratory_plots\",\"outlines_objects.rds\")\n",
    "run_efourier_pca_morphometrics_r(path_outline_objects=object_path, nharmonics=10, output_directory=working_directory,\n",
    "                                  show=True, normalize=\"False\", img_height_pca=1000, img_width_pca=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For plotting PCA\n",
    "object_path=os.path.join(working_directory,\"efourier_results\",\"pca_fourier.rds\")\n",
    "run_plot_pca_morphometrics_r(input_directory=object_path, output_directory=working_directory, PC_axis1=\"1\", PC_axis2=\"4\", img_height_pca=1000, img_width_pca=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For running kmeans\n",
    "object_path=os.path.join(working_directory,\"efourier_results\",\"pca_fourier.rds\")\n",
    "run_kmeans_efourier_r(pca_objects_path=object_path, output_directory=working_directory,max_clusters=10, img_height_pca=1000, img_width_pca=1000, plot_xlim=250, plot_ylim=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Pixel-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For run Pixel-Based PCA analysis\n",
    "process_images_and_perform_pca(directory=input_masks, working_directory=working_directory, n_components=50, k_max=10, std_multiplier=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
