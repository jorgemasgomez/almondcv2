{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORKFLOW COMPLETE FOR GOOGLE COLAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "repo_path = \"/content/drive/MyDrive/GitHub\"\n",
    "# Crear la carpeta si no existe\n",
    "os.makedirs(repo_path, exist_ok=True)\n",
    "\n",
    "# Moverse a la carpeta\n",
    "%cd {repo_path}\n",
    "\n",
    "# Clonar el repositorio (reemplaza con tu URL)\n",
    "# !git clone https://github.com/usuario/repositorio.\n",
    "!git clone https://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@github.com/jorgemasgomez/almondcv2.git\n",
    "\n",
    "%cd {repo_path}/almondcv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you have cloned the repository previously simply move to the folder \n",
    "repo_path = \"/content/drive/MyDrive/GitHub\"\n",
    "%cd {repo_path}/almondcv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements_google_colab.txt #In Google Colab will be necessary to install it in each session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pre-processing workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import os\n",
    "from calibrations import  build_calibration, calibrate_color_and_distortion, calibrate_color, calibrate_distortion\n",
    "from aux_functions import obtain_pixel_metric, ungroup_pic\n",
    "from model_class import ModelSegmentation\n",
    "import pandas as pd\n",
    "\n",
    "#Set paths of the files\n",
    "working_directory=\"C:/Users/Pheno/Documents/database_almondcv2/\"\n",
    "chessboards=os.path.join(working_directory, \"calibracion/chessboards\") #folder with chessboard pitcures\n",
    "raw_folder=os.path.join(working_directory,\"pruebas_jorge\")#folder with the pictures to calibrate\n",
    "mtx_input_path=os.path.join(chessboards,\"calibration_mtx.npz\") #for distortion in npz format\n",
    "standard_matrix_color=os.path.join(working_directory, \"pruebas_jorge/28_10_CG-009.JPG\") #picture of reference\n",
    "output_calibrated=os.path.join(working_directory,\"pruebas_jorge\") #output folder for calibrated pictures\n",
    "\n",
    "coin_model_path=os.path.join(working_directory,\"models/coin_2022_yolov11_640.pt\")\n",
    "info_table=os.path.join(working_directory,\"info_data.txt\")\n",
    "\n",
    "group_model_path=os.path.join(working_directory, \"models/rectangle_2022_yolov11s_1280.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color and Distortion calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First build your distortion model based in chessboards\n",
    "build_calibration(chessboardSize=(6, 8), frameSize=(5472,3648), dir_path=chessboards, \n",
    "                  image_format=\".jpg\", size_of_chessboard_squares_mm=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for calibrate color and distortion\n",
    "calibrate_color_and_distortion(raw_folder=raw_folder,mtx_input_path=mtx_input_path,output_calibrated=output_calibrated,\n",
    "                                radius_param=10, standard_matrix=standard_matrix_color) #Standard matrix is a picture of reference to use instead of the original picture for error cases or simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for calibrate distortion only\n",
    "\n",
    "calibrate_distortion(input_folder=raw_folder, mtx_input=mtx_input_path, output_path=output_calibrated, input_picture=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for calibrate color only\n",
    "\n",
    "calibrate_color(input_folder=raw_folder, output_path=output_calibrated,standard_matrix=standard_matrix_color,\n",
    "                 force_standard_matrix=\"No\")  #force_standard_matrix option uses in all the pictures the reference picture. In negative case, use only standard_matrix in error cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain pixel size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deploy the model \n",
    "\n",
    "reference_model=ModelSegmentation(working_directory=working_directory)\n",
    "contours_coin=reference_model.slice_predict_reconstruct(input_folder=output_calibrated, imgsz=640,\n",
    "                                                         model_path=coin_model_path, slice_height=640, slice_width=640,\n",
    "                                                         overlap_height_ratio=0.1, overlap_width_ratio=0.1,\n",
    "                                                           retina_mask=True, conf=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load info table\n",
    "info_data_df=pd.read_csv(info_table,sep=\"\\t\")\n",
    "# If we use a calibrated dataset but the info table was previous we can include CL_ automatically with this line\n",
    "# info_data_df['Name_picture'] = info_data_df['Name_picture'].apply(lambda x: 'CL_' + x)\n",
    "\n",
    "info_data_completed=obtain_pixel_metric(info_data=info_data_df, contours=contours_coin,\n",
    "                                         output_directory=working_directory, reference=24.25) #reference in mm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ungroup pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load group model\n",
    "group_model=ModelSegmentation(working_directory=working_directory)\n",
    "contours_groups=group_model.predict_model(model_path=group_model_path,\n",
    "                               folder_input=output_calibrated,\n",
    "                               imgsz=1280, check_result=False, max_det=2, retina_mask=False) #Retina mask not recommended here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain sample pictures and update info table. \n",
    "info_data_completed_path=os.path.join(working_directory, \"info_data_completed_2022 (2).txt\")\n",
    "info_data_completed=pd.read_csv(info_data_completed_path,sep=\"\\t\")\n",
    "\n",
    "ungroup_pic(input_contours=contours_groups, output_path=working_directory, info_file=info_data_completed, axis=\"X\") #axis indicate if the samples should be order according to Y or X axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Develop your segmentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import os\n",
    "from model_class import ModelSegmentation\n",
    "from aux_functions import slicing\n",
    "import cv2\n",
    "#Inputs\n",
    "working_directory=\"C:/Users/Pheno/Documents/database_almondcv2/\"\n",
    "pictures_directory=os.path.join(working_directory, \"fotos_prueba_seed_2022\")\n",
    "pre_model=os.path.join(working_directory, \"models/yolo11s-seg.pt\")\n",
    "model_path=os.path.join(working_directory, \"models/seed_2022_yolov11s_320.pt\")\n",
    "output_directory=os.path.join(working_directory,\"output_directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slice_pictures for training\n",
    "slicing(input_folder=pictures_directory,output_directory=working_directory,name_slicing=\"Slices_probando\", number_pictures=4, slice_height=320, slice_width=320)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label with CVAT\n",
    "\n",
    "zip_file_shell=os.path.join(working_directory,\"shell_2023_320.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model segmentation training\n",
    "\n",
    "model=ModelSegmentation(working_directory=working_directory)\n",
    "model.train_segmentation_model(input_zip=zip_file_shell, epochs=5,imgsz=320, name_segmentation=\"shell_2023_320\",\n",
    "                                      pre_model=pre_model, batch=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy and reconstruct a picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slice_predict_reconstruct approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join patches approach\n",
    "\n",
    "model=ModelSegmentation(working_directory=working_directory)\n",
    "masks=model.slice_predict_reconstruct(input_folder=pictures_directory,imgsz=320, model_path=model_path,\n",
    "                                          slice_height=320, slice_width=320,overlap_height_ratio=0.2,\n",
    "                                          overlap_width_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show the masks\n",
    "\n",
    "for mask in masks:\n",
    "    cv2.imwrite(f\"{output_directory}/{os.path.basename(mask[1])}\", mask[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ModelSegmentation(working_directory=working_directory)\n",
    "masks=model.predict_model_sahi(model_path=model_path, check_result=False, folder_input=pictures_directory,\n",
    "                                            retina_masks=True,\n",
    "                                              postprocess_match_threshold=0.2, overlap_height_ratio=0.2,\n",
    "                                                overlap_width_ratio=0.2, postprocess_match_metric=\"IOS\", \n",
    "                                                postprocess_type=\"GREEDYNMM\", slice_height=320, slice_width=320,\n",
    "                                                  confidence_treshold=0.95,\n",
    "                                                  imgsz=320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show the masks\n",
    "for mask in masks:\n",
    "    mask[0].export_visuals(export_dir=output_directory, hide_labels=True, rect_th=1, file_name=f\"{os.path.basename(mask[1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deploy your segmentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import os\n",
    "from model_class import ModelSegmentation\n",
    "import pickle\n",
    "from pictures_class import Pictures\n",
    "import pandas as pd\n",
    "#Inputs\n",
    "working_directory=\"C:/Users/Pheno/Documents/database_almondcv2/\"\n",
    "pictures_directory=os.path.join(working_directory, \"fotos_prueba_seed_2022\")\n",
    "model_path=os.path.join(working_directory, \"models/seed_2022_yolov11s_320.pt\")\n",
    "info_data_completed_path=os.path.join(working_directory, \"info_data_completed_ungrouped_2022.txt\")\n",
    "info_data_completed=pd.read_csv(info_data_completed_path,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose your reconstruction approach and measure (almond)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice predict reconstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/05/2025 13:30:58 - INFO - sahi.slicing -   image.shape: (756, 2934)\n",
      "02/05/2025 13:30:58 - INFO - sahi.slicing -   Num slices: 36 slice_height: 320 slice_width: 320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected GPU: NVIDIA GeForce RTX 3060\n",
      "Total GPU Memory: 12.00 GB\n",
      "Image 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/05/2025 13:31:05 - INFO - sahi.slicing -   image.shape: (756, 2951)\n",
      "02/05/2025 13:31:05 - INFO - sahi.slicing -   Num slices: 36 slice_height: 320 slice_width: 320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 2/2\n"
     ]
    }
   ],
   "source": [
    "# Join patches approach\n",
    "\n",
    "model=ModelSegmentation(working_directory=working_directory)\n",
    "masks=model.slice_predict_reconstruct(input_folder=pictures_directory,imgsz=320, model_path=model_path,\n",
    "                                          slice_height=320, slice_width=320,overlap_height_ratio=0.2,\n",
    "                                          overlap_width_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pheno\\OneDrive - UNIVERSIDAD DE MURCIA\\Escritorio\\Almond_CV\\almondcv2\\pictures_class.py:544: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  morphology_table = pd.concat([morphology_table, row], ignore_index=True)\n",
      "c:\\Users\\Pheno\\OneDrive - UNIVERSIDAD DE MURCIA\\Escritorio\\Almond_CV\\almondcv2\\pictures_class.py:557: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  general_table=pd.concat([general_table,row_general], ignore_index=True)\n",
      "c:\\Users\\Pheno\\OneDrive - UNIVERSIDAD DE MURCIA\\Escritorio\\Almond_CV\\almondcv2\\pictures_class.py:590: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  binary_table['Binary_mask_picture'] = binary_table['Sample_picture'] + '_' + binary_table['Fruit_number'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "## Example with slice predict reconstruct approach\n",
    "pictures_object=Pictures(working_directory=working_directory, input_folder=pictures_directory,info_file=info_data_completed,\n",
    "                      fruit=\"Shell_almond\", binary_masks=True, project_name=\"probando_watershed\", blurring_binary_masks=False)\n",
    "pictures_object.set_postsegmentation_parameters(sahi=False, segmentation_input=masks, smoothing=False, smoothing_iterations=2, kernel_smoothing=3,\n",
    "                        watershed=True, kernel_watershed=5, threshold_watershed=0.6)\n",
    "pictures_object.measure_almonds(margin=400)\n",
    "\n",
    "# Save\n",
    "with open('pictures_object_watershed.pkl', 'wb') as file:\n",
    "    pickle.dump(pictures_object, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected GPU: NVIDIA GeForce RTX 3060\n",
      "Total GPU Memory: 12.00 GB\n",
      "Pic 1/2\n",
      "Performing prediction on 36 slices.\n",
      "Pic 2/2\n",
      "Performing prediction on 36 slices.\n"
     ]
    }
   ],
   "source": [
    "model=ModelSegmentation(working_directory=working_directory)\n",
    "masks=model.predict_model_sahi(model_path=model_path, check_result=False, folder_input=pictures_directory,\n",
    "                                            retina_masks=True,\n",
    "                                              postprocess_match_threshold=0.2, overlap_height_ratio=0.2,\n",
    "                                                overlap_width_ratio=0.2, postprocess_match_metric=\"IOS\", \n",
    "                                                postprocess_type=\"GREEDYNMM\", slice_height=320, slice_width=320,\n",
    "                                                  confidence_treshold=0.95,\n",
    "                                                  imgsz=320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pheno\\OneDrive - UNIVERSIDAD DE MURCIA\\Escritorio\\Almond_CV\\almondcv2\\pictures_class.py:544: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  morphology_table = pd.concat([morphology_table, row], ignore_index=True)\n",
      "c:\\Users\\Pheno\\OneDrive - UNIVERSIDAD DE MURCIA\\Escritorio\\Almond_CV\\almondcv2\\pictures_class.py:557: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  general_table=pd.concat([general_table,row_general], ignore_index=True)\n",
      "c:\\Users\\Pheno\\OneDrive - UNIVERSIDAD DE MURCIA\\Escritorio\\Almond_CV\\almondcv2\\pictures_class.py:590: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  binary_table['Binary_mask_picture'] = binary_table['Sample_picture'] + '_' + binary_table['Fruit_number'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "## Example with SAHI approach\n",
    "pictures_object=Pictures(working_directory=working_directory, input_folder=pictures_directory,info_file=info_data_completed,\n",
    "                      fruit=\"Shell_almond\", binary_masks=True, project_name=\"probando\",  blurring_binary_masks=False)\n",
    "pictures_object.set_postsegmentation_parameters(sahi=True, segmentation_input=masks)\n",
    "pictures_object.measure_almonds(margin=400)\n",
    "\n",
    "# Guardar el objeto en un archivo\n",
    "with open('pictures_object_sahi.pkl', 'wb') as file:\n",
    "    pickle.dump(pictures_object, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose your reconstruction approach and measure (general)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice predict reconstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join patches approach\n",
    "\n",
    "model=ModelSegmentation(working_directory=working_directory)\n",
    "masks=model.slice_predict_reconstruct(input_folder=pictures_directory,imgsz=320, model_path=model_path,\n",
    "                                          slice_height=320, slice_width=320,overlap_height_ratio=0.2,\n",
    "                                          overlap_width_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example with slice predict reconstruct approach\n",
    "pictures_object=Pictures(working_directory=working_directory, input_folder=pictures_directory,info_file=info_data_completed,\n",
    "                      fruit=\"Shell_almond\", binary_masks=True, project_name=\"probando_watershed\", blurring_binary_masks=False)\n",
    "pictures_object.set_postsegmentation_parameters(sahi=False, segmentation_input=masks, smoothing=False, smoothing_iterations=2, kernel_smoothing=3,\n",
    "                        watershed=True, kernel_watershed=5, threshold_watershed=0.6)\n",
    "pictures_object.measure_almonds(margin=400)\n",
    "\n",
    "# Save\n",
    "with open('pictures_object_watershed.pkl', 'wb') as file:\n",
    "    pickle.dump(pictures_object, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ModelSegmentation(working_directory=working_directory)\n",
    "masks=model.predict_model_sahi(model_path=model_path, check_result=False, folder_input=pictures_directory,\n",
    "                                            retina_masks=True,\n",
    "                                              postprocess_match_threshold=0.2, overlap_height_ratio=0.2,\n",
    "                                                overlap_width_ratio=0.2, postprocess_match_metric=\"IOS\", \n",
    "                                                postprocess_type=\"GREEDYNMM\", slice_height=320, slice_width=320,\n",
    "                                                  confidence_treshold=0.95,\n",
    "                                                  imgsz=320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example with SAHI approach\n",
    "pictures_object=Pictures(working_directory=working_directory, input_folder=pictures_directory,info_file=info_data_completed,\n",
    "                      fruit=\"Shell_almond\", binary_masks=True, project_name=\"Shell_2022_07012025_sahi\",  blurring_binary_masks=False)\n",
    "pictures_object.set_postsegmentation_parameters(sahi=True, segmentation_input=masks)\n",
    "pictures_object.measure_almonds(margin=400)\n",
    "\n",
    "# Guardar el objeto en un archivo\n",
    "with open('pictures_object_sahi.pkl', 'wb') as file:\n",
    "    pickle.dump(pictures_object, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Morphometrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import os\n",
    "from morphometrics_functions import install_morphometrics_packages_r, exploratory_morphometrics_r, run_efourier_pca_morphometrics_r, run_plot_pca_morphometrics_r, run_kmeans_efourier_r, process_images_and_perform_pca\n",
    "\n",
    "#Inputs\n",
    "\n",
    "input_masks=r\"C:\\Users\\Pheno\\Documents\\database_almondcv2\\BACKUPS_Resultados_postnavidad\\clean\\pruebas\"\n",
    "working_directory=r\"C:\\Users\\Pheno\\Documents\\database_almondcv2\\MORPHOMETRICS\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For installing libraries\n",
    "install_morphometrics_packages_r()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For exploring the dataset\n",
    "exploratory_morphometrics_r(info_data=\"\", grouping_factor=\"\", input_directory=input_masks,\n",
    "                             output_directory=working_directory, show=True, nharmonics=10,nexamples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For running EFA and PCA \n",
    "object_path=os.path.join(working_directory,\"exploratory_plots\",\"outlines_objects.rds\")\n",
    "run_efourier_pca_morphometrics_r(path_outline_objects=object_path, nharmonics=10, output_directory=working_directory,\n",
    "                                  show=True, normalize=\"False\", img_height_pca=1000, img_width_pca=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For plotting PCA\n",
    "object_path=os.path.join(working_directory,\"efourier_results\",\"pca_fourier.rds\")\n",
    "run_plot_pca_morphometrics_r(input_directory=object_path, output_directory=working_directory, PC_axis1=\"1\", PC_axis2=\"4\", img_height_pca=1000, img_width_pca=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For running kmeans\n",
    "object_path=os.path.join(working_directory,\"efourier_results\",\"pca_fourier.rds\")\n",
    "run_kmeans_efourier_r(pca_objects_path=object_path, output_directory=working_directory,max_clusters=10, img_height_pca=1000, img_width_pca=1000, plot_xlim=250, plot_ylim=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Pixel-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For run Pixel-Based PCA analysis\n",
    "process_images_and_perform_pca(directory=input_masks, working_directory=working_directory, n_components=50, k_max=10, std_multiplier=3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
